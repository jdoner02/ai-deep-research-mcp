"""
Enhanced Arbitrary Query Research System - Web-Based Multi-Source Research
Handles any research query with dynamic web search and content retrieval
"""
import asyncio
import json
import sys
import tempfile
from pathlib import Path
import logging
import time
from typing import List, Dict, Any

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import all required components  
from src.web_search import WebSearcher
from src.content_loaders import MultiSourceContentLoader
from src.document_parser import DocumentParser
from src.embedder import Embedder, TextChunk, EmbeddedChunk
from src.vector_store import VectorStore

class ArbitraryQueryResearcherWeb:
    """Research system that can handle any query dynamically using web search"""
    
    def __init__(self, max_sources: int = 10):
        self.max_sources = max_sources
        self.web_searcher = WebSearcher()
        self.content_loader = MultiSourceContentLoader()
        self.document_parser = DocumentParser()
        self.embedder = Embedder()
        
        # Create temporary directory for vector store
        import tempfile
        self.temp_dir = tempfile.mkdtemp()
        self.vector_store = VectorStore(self.temp_dir)
        
        self.setup_logging()
        
    def generate_search_queries(self, query: str) -> List[str]:
        """Generate multiple search query variations for comprehensive research"""
        base_query = query.strip()
        
        # For AP Cyber specifically, be more specific to avoid AP News
        if 'ap cyber' in base_query.lower():
            search_queries = [
                "Advanced Placement cybersecurity curriculum",
                "AP Computer Science Principles cybersecurity units",
                "high school cybersecurity course AP exam",
                "College Board cybersecurity curriculum",
                "AP CSP cyber security lesson plans",
                "cybersecurity education high school curriculum"
            ]
        else:
            # Generate search variations for other queries
            search_queries = [
                base_query,
                f"{base_query} explained",
                f"{base_query} guide tutorial",
                f"{base_query} overview summary",
                f"what is {base_query}",
                f"{base_query} curriculum course"
            ]
        
        return search_queries[:6]  # Limit to 6 variations
    
    def search_web_sources(self, query: str) -> List[Dict]:
        """Search the web for relevant sources"""
        print(f"üåê Searching web sources for: '{query}'")
        
        # Generate search query variations
        search_queries = self.generate_search_queries(query)
        
        all_sources = []
        for search_query in search_queries:
            try:
                results = self.web_searcher.search(search_query, max_results=3)
                all_sources.extend(results)
                time.sleep(1)  # Be polite to search engine
            except Exception as e:
                print(f"‚ùå Search failed for '{search_query}': {e}")
                continue
        
        # Remove duplicates and limit results  
        seen_urls = set()
        unique_sources = []
        for source in all_sources:
            url = source.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_sources.append(source)
        
        return unique_sources[:self.max_sources]
    
    def load_and_process_sources(self, sources: List[Dict]) -> List[Dict]:
        """Load content from web sources and process into documents"""
        print(f"üìÑ Loading content from {len(sources)} web sources...")
        
        # Extract URLs from search results
        urls = [source['url'] for source in sources if source.get('url')]
        
        # Load content from URLs
        documents = self.content_loader.load_multiple_urls(urls)
        
        # Parse and chunk the documents
        processed_docs = []
        for doc in documents:
            try:
                # Parse the document (mainly for chunking)
                parsed_doc = self.document_parser.parse_text(doc['text'], doc['source'])
                
                # Extract chunks from parsed document
                if hasattr(parsed_doc, 'chunks'):
                    chunks = parsed_doc.chunks
                elif isinstance(parsed_doc, dict) and 'chunks' in parsed_doc:
                    chunks = parsed_doc['chunks']
                else:
                    # Fallback: create simple chunks
                    text = doc['text']
                    chunk_size = 1000
                    chunks = []
                    for i in range(0, len(text), chunk_size):
                        chunk_text = text[i:i+chunk_size]
                        chunks.append({
                            'content': chunk_text,
                            'source': doc['source'],
                            'chunk_id': f"chunk_{i//chunk_size}"
                        })
                
                processed_doc = {
                    'title': doc['title'],
                    'source': doc['source'],
                    'type': doc['type'],
                    'chunks': chunks,
                    'metadata': {
                        'length': doc['length'],
                        'chunk_count': len(chunks)
                    }
                }
                processed_docs.append(processed_doc)
                
            except Exception as e:
                print(f"‚ùå Failed to process document from {doc['source']}: {e}")
                continue
        
        return processed_docs
    
    def index_documents(self, documents: List[Dict]) -> Dict:
        """Index documents in vector store for semantic search"""
        print(f"üîç Indexing {len(documents)} documents...")
        
        all_chunks = []
        for doc in documents:
            for i, chunk in enumerate(doc['chunks']):
                # Create a proper TextChunk first
                text_chunk = TextChunk(
                    text=chunk['content'],
                    source_url=doc['source'],
                    chunk_id=f"{doc['source']}_chunk_{i}",
                    metadata={
                        'title': doc['title'],
                        'type': doc['type'],
                        'source_title': doc['title']
                    },
                    start_char=i * 1000,  # Approximate
                    end_char=(i + 1) * 1000  # Approximate
                )
                all_chunks.append(text_chunk)
        
        if not all_chunks:
            return {'chunks_indexed': 0, 'documents_processed': 0}
        
        # Generate embeddings and create EmbeddedChunk objects
        embedded_chunks = []
        for chunk in all_chunks:
            try:
                embedding = self.embedder.get_embedding(chunk.text)
                embedded_chunk = EmbeddedChunk(
                    text=chunk.text,
                    source_url=chunk.source_url,
                    chunk_id=chunk.chunk_id,
                    metadata=chunk.metadata,
                    start_char=chunk.start_char,
                    end_char=chunk.end_char,
                    embedding=embedding,
                    embedding_model=self.embedder.embedding_model
                )
                embedded_chunks.append(embedded_chunk)
            except Exception as e:
                print(f"‚ùå Failed to generate embedding: {e}")
                continue
        
        # Store in vector database
        if embedded_chunks:
            self.vector_store.add_chunks(embedded_chunks)
        
        return {
            'chunks_indexed': len(embedded_chunks),
            'documents_processed': len(documents)
        }
    
    def retrieve_relevant_content(self, query: str, top_k: int = 5) -> List[Dict]:
        """Retrieve most relevant content chunks for the query"""
        print(f"üéØ Retrieving relevant content for: '{query}'")
        
        try:
            # Generate query embedding
            query_embedding = self.embedder.get_embedding(query)
            
            # Search vector store
            search_results = self.vector_store.search_by_vector(query_embedding, top_k=top_k)
            
            # Convert SearchResult objects to dictionaries
            results = []
            for result in search_results:
                result_dict = {
                    'content': result.text,
                    'source': result.source_url,
                    'source_title': result.metadata.get('title', 'Unknown Document'),
                    'score': result.score,
                    'metadata': result.metadata
                }
                results.append(result_dict)
            
            return results
            
        except Exception as e:
            print(f"‚ùå Content retrieval failed: {e}")
            return []
    
    def calculate_relevance(self, query: str, content: str) -> float:
        """Calculate relevance score between query and content"""
        query_words = set(query.lower().split())
        content_words = set(content.lower().split())
        
        if not query_words:
            return 0.0
        
        # Simple word overlap scoring
        overlap = len(query_words.intersection(content_words))
        relevance = overlap / len(query_words)
        
        return min(relevance, 1.0)
    
    def generate_answer(self, query: str, relevant_content: List[Dict]) -> str:
        """Generate comprehensive answer from relevant content"""
        if not relevant_content:
            return f"No relevant information found for query: '{query}'"
        
        # Compile information from relevant chunks
        info_parts = []
        sources_used = set()
        
        for item in relevant_content:
            content = item.get('content', '')
            source = item.get('source', 'Unknown')
            source_title = item.get('source_title', 'Unknown Document')
            
            if content and len(content) > 50:  # Only include substantial content
                info_parts.append(f"From {source_title}: {content}")
                sources_used.add(f"{source_title} ({source})")
        
        if not info_parts:
            return f"Insufficient relevant content found for query: '{query}'"
        
        # Compile answer
        answer_parts = [
            f"Research findings for '{query}':\n",
            "\n".join(info_parts[:5]),  # Limit to top 5 pieces of content
            f"\n\nThis information was compiled from {len(sources_used)} web sources."
        ]
        
        return "\n".join(answer_parts)
    
    def research_query(self, query: str) -> Dict[str, Any]:
        """Main research method - handles any arbitrary query"""
        print(f"üî¨ Starting research for query: '{query}'")
        return self._internal_research_query(query)
        
    def expand_web_query(self, query: str) -> List[str]:
        """Expand query with web-specific terms"""
        base_queries = self.generate_search_queries(query)
        web_expansions = []
        
        for base_query in base_queries:
            web_expansions.extend([
                f"{base_query} online",
                f"{base_query} web",
                f"{base_query} website",
                f"{base_query} guide"
            ])
        
        return list(set(base_queries + web_expansions))  # Remove duplicates
        
    def should_process_content_type(self, content_type: str) -> bool:
        """Check if content type should be processed"""
        processable_types = [
            "text/html",
            "application/pdf", 
            "text/plain",
            "application/json",
            "text/xml"
        ]
        return any(ptype in content_type.lower() for ptype in processable_types)
        
    def collect_web_statistics(self, stats_data: Dict) -> Dict:
        """Collect and format web research statistics"""
        formatted_stats = stats_data.copy()
        
        if 'urls_processed' in stats_data and 'successful_fetches' in stats_data:
            total = stats_data['urls_processed']
            success = stats_data['successful_fetches']
            formatted_stats['success_rate'] = (success / total * 100) if total > 0 else 0
            
        return formatted_stats
        
    def setup_logging(self):
        """Setup logging that won't interfere with JSON output"""
        import logging
        logging.basicConfig(
            level=logging.WARNING,
            format='%(message)s'
        )
        
        # Suppress specific loggers
        logging.getLogger('src.embedder').setLevel(logging.CRITICAL)
        logging.getLogger('src.vector_store').setLevel(logging.CRITICAL)
        logging.getLogger('chromadb').setLevel(logging.CRITICAL)
        
    async def search_and_research(self, query: str, max_results: int = None) -> Dict[str, Any]:
        """Alias for research_query to match test expectations"""
        if max_results:
            original_max = self.max_sources
            self.max_sources = max_results
            result = self.research_query(query)
            self.max_sources = original_max
            return result
        return self.research_query(query)
    
    def _internal_research_query(self, query: str) -> Dict[str, Any]:
        """Internal method implementing research logic"""
        start_time = time.time()
        
        try:
            # Step 1: Search web for relevant sources
            web_sources = self.search_web_sources(query)
            if not web_sources:
                return {
                    'success': False,
                    'error': 'No web sources found',
                    'query': query
                }
            
            # Step 2: Load and process content from sources
            documents = self.load_and_process_sources(web_sources)
            if not documents:
                return {
                    'success': False,
                    'error': 'No content loaded from sources',
                    'query': query,
                    'sources_found': len(web_sources)
                }
            
            # Step 3: Index documents for semantic search
            index_stats = self.index_documents(documents)
            
            # Step 4: Retrieve relevant content
            relevant_content = self.retrieve_relevant_content(query)
            
            # Step 5: Generate comprehensive answer
            answer = self.generate_answer(query, relevant_content)
            
            # Compile sources information
            sources = []
            for doc in documents:
                sources.append({
                    'title': doc['title'],
                    'url': doc['source'],
                    'type': doc['type'],
                    'chunks': len(doc['chunks'])
                })
            
            end_time = time.time()
            
            result = {
                'success': True,
                'query': query,
                'answer': answer,
                'sources': sources,
                'statistics': {
                    'search_queries_generated': len(self.generate_search_queries(query)),
                    'web_sources_found': len(web_sources),
                    'documents_processed': index_stats['documents_processed'],
                    'chunks_indexed': index_stats['chunks_indexed'],
                    'relevant_results': len(relevant_content),
                    'processing_time': round(end_time - start_time, 2)
                }
            }
            
            return result
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'query': query
            }

# CLI interface for testing
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python enhanced_query_research.py '<query>'")
        sys.exit(1)
    
    query = sys.argv[1]
    researcher = ArbitraryQueryResearcher()
    
    print(f"üß† AI Deep Research MCP - Enhanced Query Research")
    print(f"üîç Query: {query}")
    print("=" * 60)
    
    result = researcher.research_query(query)
    
    # Output result as JSON for web interface
    print("RESULT_START")
    print(json.dumps(result, indent=2))
    print("RESULT_END")
