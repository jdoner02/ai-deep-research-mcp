# üìÑ Document Processing and Text Extraction - Educational Module

## Welcome to Document Processing!

Imagine you're a research assistant who needs to read hundreds of research papers, web articles, and PDF documents to answer a complex question. That's exactly what AI research systems do - but they need to "read" and "understand" documents in a very different way than humans do.

In this educational module, we'll learn how to test document processing components, understand text extraction techniques, and explore how AI systems convert messy real-world documents into clean, structured data.

## üìö What is Document Processing?

Document processing is like having a super-efficient reading assistant that:
1. **Takes any type of document** (HTML, PDF, Word, etc.)
2. **Extracts the important text** while ignoring formatting, ads, and navigation
3. **Structures the information** with titles, content, metadata
4. **Cleans and normalizes** the text for AI analysis
5. **Preserves source information** for citations and verification

### Real-World Example: From Messy Web Page to Clean Text

When an AI system processes a news article webpage, here's what happens:

**Input** (Raw HTML):
```html
<html>
<body>
  <nav>Home | About | Contact</nav>
  <div class="ads">Buy our product!</div>
  <article>
    <h1>Breakthrough in Quantum Computing</h1>
    <p>Scientists at MIT have achieved...</p>
    <p>This discovery could revolutionize...</p>
  </article>
  <footer>Copyright 2025</footer>
</body>
</html>
```

**Output** (Processed Document):
```python
ParsedDocument(
    title="Breakthrough in Quantum Computing",
    content="Scientists at MIT have achieved... This discovery could revolutionize...",
    source_url="https://news-site.com/quantum-breakthrough",
    metadata={"author": "Dr. Smith", "date": "2025-07-31"},
    content_type="text/html"
)
```

The processor removed navigation, ads, and footer - keeping only the valuable content!

## üß™ Testing Document Processors: Handling the Messy Real World

### Why Test Document Processing?
Documents in the wild are messy and unpredictable:
- **Inconsistent formats**: Every website structures content differently
- **Hidden content**: Some text might be loaded by JavaScript
- **Encoding issues**: Special characters, different languages
- **File corruption**: PDFs might be password-protected or malformed
- **Mixed content**: Images, tables, mathematical formulas
- **Size variations**: From tiny snippets to massive research papers

Testing helps us build processors that handle this chaos gracefully.

### The TDD Approach for Document Processing

Let's learn by building comprehensive tests for a document processor:

#### üî¥ RED Phase: Write Failing Tests First

```python
import pytest
from typing import Dict, List, Optional
from unittest.mock import Mock, patch, mock_open
from pathlib import Path
import tempfile

# These imports will fail initially - that's expected in TDD!
try:
    from src.document_parser import DocumentParser, ParsedDocument
except ImportError:
    DocumentParser = None
    ParsedDocument = None

class TestDocumentParser:
    """
    üìÑ Test suite for DocumentParser component.
    
    We define what we want our document processor to do before
    we build it. This ensures we think about edge cases and
    requirements upfront.
    
    üéØ Learning Goals:
    - Understand how to test file I/O operations
    - Learn about testing different content types
    - Practice using fixtures for test data
    - See how to test text processing and extraction
    """
    
    def test_document_parser_can_be_created(self):
        """
        üèóÔ∏è Test that we can create a DocumentParser instance.
        
        Starting simple - can we make our object and configure it?
        
        üí° Configuration options might include:
        - Which file types to support
        - How aggressive to be with content extraction
        - Whether to preserve formatting
        - Language detection settings
        """
        assert DocumentParser is not None, "DocumentParser class should exist"
        
        # Test default configuration
        parser = DocumentParser()
        assert parser is not None
        assert isinstance(parser, DocumentParser)
        
        # Test custom configuration
        parser_custom = DocumentParser(
            extract_images=False,
            preserve_formatting=True,
            max_content_length=10000
        )
        assert parser_custom is not None

    def test_parsed_document_structure(self):
        """
        üìã Test that ParsedDocument contains all required information.
        
        When we process a document, we need structured output that
        includes both the content and metadata about where it came from.
        
        üí° Think of this like a library card catalog:
        - Title (what is this document about?)
        - Content (the actual text)
        - Source URL (where did we find it?)
        - Metadata (author, date, language, etc.)
        - Content type (HTML, PDF, Word doc, etc.)
        """
        assert ParsedDocument is not None, "ParsedDocument should exist"
        
        # Test creating a complete parsed document
        doc = ParsedDocument(
            title="Understanding Machine Learning",
            content="Machine learning is a subset of artificial intelligence...",
            source_url="https://education-site.com/ml-guide",
            metadata={
                "author": "Dr. Jane Smith",
                "publication_date": "2025-07-31",
                "language": "en",
                "word_count": 1250
            },
            content_type="text/html"
        )
        
        # Verify all fields are accessible and correct
        assert doc.title == "Understanding Machine Learning"
        assert "artificial intelligence" in doc.content
        assert doc.source_url.startswith("https://")
        assert doc.metadata["author"] == "Dr. Jane Smith"
        assert doc.content_type == "text/html"

    def test_parse_html_extracts_title_and_content(self):
        """
        üåê Test HTML parsing extracts the main content correctly.
        
        HTML pages have lots of extra stuff (navigation, ads, footers).
        Our parser should extract just the important content.
        
        üí° HTML parsing challenges:
        - Multiple title tags or no title tag
        - Content spread across different elements
        - Navigation and sidebar content to ignore
        - Embedded scripts and styles to remove
        """
        parser = DocumentParser()
        
        # Sample HTML with typical webpage structure
        sample_html = '''
        <!DOCTYPE html>
        <html>
        <head>
            <title>The Future of Renewable Energy</title>
            <meta name="author" content="Energy Expert">
        </head>
        <body>
            <nav>
                <a href="/home">Home</a>
                <a href="/about">About</a>
            </nav>
            
            <main>
                <article>
                    <h1>The Future of Renewable Energy</h1>
                    <p>Solar and wind power are becoming increasingly affordable...</p>
                    <p>By 2030, renewable sources could provide 50% of global electricity...</p>
                    
                    <h2>Current Challenges</h2>
                    <p>Despite progress, several challenges remain...</p>
                </article>
            </main>
            
            <aside>
                <div class="advertisement">Buy solar panels now!</div>
            </aside>
            
            <footer>Copyright 2025 Energy News</footer>
        </body>
        </html>
        '''
        
        # Test parsing the HTML
        result = parser.parse_html(sample_html, source_url="https://energy-news.com/renewable-future")
        
        # Verify title extraction
        assert result.title == "The Future of Renewable Energy"
        
        # Verify main content extraction (should include article text)
        assert "Solar and wind power" in result.content
        assert "By 2030, renewable sources" in result.content
        assert "Current Challenges" in result.content
        assert "several challenges remain" in result.content
        
        # Verify unwanted content is excluded
        assert "Home" not in result.content  # Navigation
        assert "About" not in result.content  # Navigation
        assert "Buy solar panels" not in result.content  # Advertisement
        assert "Copyright 2025" not in result.content  # Footer
        
        # Verify metadata
        assert result.source_url == "https://energy-news.com/renewable-future"
        assert result.content_type == "text/html"

    def test_parse_html_handles_missing_title(self):
        """
        ‚ùì Test HTML parsing when title tag is missing.
        
        Not all HTML pages have proper title tags. Our parser should
        handle this gracefully, maybe using the first heading instead.
        
        üí° Fallback strategies:
        - Use first <h1> tag as title
        - Use meta description
        - Use filename if it's a local file
        - Use "Untitled Document" as last resort
        """
        parser = DocumentParser()
        
        # HTML without a title tag
        html_without_title = '''
        <html>
        <body>
            <h1>This Should Be The Title</h1>
            <p>This is the main content of the document.</p>
        </body>
        </html>
        '''
        
        result = parser.parse_html(html_without_title)
        
        # Should use first h1 as title
        assert result.title == "This Should Be The Title"
        assert "main content" in result.content

    def test_parse_pdf_extracts_text(self):
        """
        üìÑ Test PDF parsing extracts text content.
        
        PDFs are tricky because they're designed for printing, not for
        easy text extraction. Text might be in weird orders, have
        formatting issues, or be embedded as images.
        
        üí° PDF challenges:
        - Text might not be in reading order
        - Multiple columns need to be handled properly  
        - Images with text (need OCR)
        - Password-protected files
        - Corrupted or malformed PDFs
        """
        parser = DocumentParser()
        
        # We'll mock the PDF content since we can't include real PDFs in tests
        mock_pdf_content = "This is extracted text from a PDF document. It contains research findings about artificial intelligence."
        
        # Mock the PDF parsing library
        with patch('PyPDF2.PdfReader') as mock_pdf_reader:
            # Configure mock to return our test content
            mock_page = Mock()
            mock_page.extract_text.return_value = mock_pdf_content
            
            mock_reader = Mock()
            mock_reader.pages = [mock_page]
            mock_pdf_reader.return_value = mock_reader
            
            # Test parsing a PDF file
            with tempfile.NamedTemporaryFile(suffix='.pdf') as temp_pdf:
                result = parser.parse_pdf(temp_pdf.name)
                
                # Verify content extraction
                assert "extracted text from a PDF" in result.content
                assert "artificial intelligence" in result.content
                assert result.content_type == "application/pdf"
                assert result.title is not None  # Should extract or generate a title

    def test_parse_pdf_handles_corrupted_file(self):
        """
        üí• Test PDF parsing handles corrupted or invalid files.
        
        Real-world PDFs can be corrupted, password-protected, or
        not actually PDFs at all. Our parser should handle these
        gracefully without crashing.
        
        üí° Error handling strategies:
        - Try multiple PDF parsing libraries
        - Return partial content if possible
        - Log errors for debugging
        - Continue processing other documents
        """
        parser = DocumentParser()
        
        # Mock a corrupted PDF that raises an exception
        with patch('PyPDF2.PdfReader') as mock_pdf_reader:
            mock_pdf_reader.side_effect = Exception("PDF is corrupted or encrypted")
            
            # Test parsing a corrupted PDF
            with tempfile.NamedTemporaryFile(suffix='.pdf') as temp_pdf:
                result = parser.parse_pdf(temp_pdf.name)
                
                # Should handle error gracefully
                assert result is not None
                assert result.content == ""  # No content extracted
                assert "error" in result.metadata or result.title == "Error: Could not parse PDF"

    def test_parse_handles_different_encodings(self):
        """
        üåç Test parsing handles different text encodings.
        
        Documents from around the world use different character encodings.
        Our parser should detect and handle these properly.
        
        üí° Common encodings:
        - UTF-8 (standard for modern web)
        - ISO-8859-1 (Western European)
        - Windows-1252 (Windows default)
        - Various Asian encodings (GB2312, Shift-JIS, etc.)
        """
        parser = DocumentParser()
        
        # Test text with special characters
        test_cases = [
            ("Hello, world!", "ascii"),  # Simple ASCII
            ("Caf√© r√©sum√© na√Øve", "utf-8"),  # French accents
            ("¬°Hola! ¬øC√≥mo est√°s?", "utf-8"),  # Spanish characters
            ("–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π –º–∏—Ä", "utf-8"),  # Cyrillic (Russian)
            ("„Åì„Çì„Å´„Å°„ÅØ‰∏ñÁïå", "utf-8"),  # Japanese
        ]
        
        for text, encoding in test_cases:
            # Create HTML with the test text
            html_content = f'''
            <html>
            <head><title>Test Document</title></head>
            <body><p>{text}</p></body>
            </html>
            '''
            
            # Parse the content
            result = parser.parse_html(html_content)
            
            # Verify the text was preserved correctly
            assert text in result.content
            assert result.title == "Test Document"

    def test_parse_extracts_metadata(self):
        """
        üìä Test parsing extracts useful metadata from documents.
        
        Metadata helps us understand and cite documents properly.
        It's like the "nutrition label" of a document.
        
        üí° Useful metadata includes:
        - Author and publication info
        - Creation/modification dates
        - Language detection
        - Word count and reading time
        - Keywords and topics
        """
        parser = DocumentParser(extract_metadata=True)
        
        # HTML with rich metadata
        html_with_metadata = '''
        <html>
        <head>
            <title>Climate Change Research Update</title>
            <meta name="author" content="Dr. Climate Researcher">
            <meta name="description" content="Latest findings on global warming trends">
            <meta name="keywords" content="climate, environment, research, science">
            <meta name="publication-date" content="2025-07-31">
            <meta property="og:type" content="article">
        </head>
        <body>
            <article>
                <h1>Climate Change Research Update</h1>
                <p>Recent studies show that global temperatures continue to rise...</p>
                <p>The implications for coastal cities are significant...</p>
                <p>Researchers recommend immediate action on carbon emissions...</p>
            </article>
        </body>
        </html>
        '''
        
        result = parser.parse_html(html_with_metadata)
        
        # Verify metadata extraction
        assert result.metadata["author"] == "Dr. Climate Researcher"
        assert result.metadata["description"] == "Latest findings on global warming trends"
        assert "climate" in result.metadata["keywords"]
        assert result.metadata["publication_date"] == "2025-07-31"
        
        # Verify computed metadata
        assert "word_count" in result.metadata
        assert result.metadata["word_count"] > 0
        assert "language" in result.metadata  # Should detect English

    def test_parse_handles_large_documents(self):
        """
        üìè Test parsing handles very large documents efficiently.
        
        Some documents (like research papers or technical manuals)
        can be huge. Our parser should handle them without running
        out of memory or taking forever.
        
        üí° Large document strategies:
        - Stream processing instead of loading everything into memory
        - Truncate content if it exceeds limits
        - Process in chunks
        - Provide progress updates for very large files
        """
        parser = DocumentParser(max_content_length=1000)  # Limit for testing
        
        # Create a large HTML document
        large_content = "<p>" + "This is a very long paragraph. " * 200 + "</p>"
        large_html = f'''
        <html>
        <head><title>Very Large Document</title></head>
        <body>{large_content}</body>
        </html>
        '''
        
        result = parser.parse_html(large_html)
        
        # Should handle large content appropriately
        assert result.title == "Very Large Document"
        assert len(result.content) <= 1000  # Should be truncated
        assert "truncated" in result.metadata.get("processing_notes", "").lower()

    def test_parse_batch_processes_multiple_documents(self):
        """
        üìö Test batch processing multiple documents efficiently.
        
        In research, we often need to process hundreds or thousands
        of documents. Batch processing should be efficient and robust.
        
        üí° Batch processing considerations:
        - Process in parallel when possible
        - Handle individual failures without stopping the batch
        - Provide progress tracking
        - Memory management for large batches
        """
        parser = DocumentParser()
        
        # Create multiple test documents
        test_documents = [
            ("doc1.html", "<html><head><title>Document 1</title></head><body><p>Content 1</p></body></html>"),
            ("doc2.html", "<html><head><title>Document 2</title></head><body><p>Content 2</p></body></html>"),
            ("doc3.html", "<html><head><title>Document 3</title></head><body><p>Content 3</p></body></html>"),
        ]
        
        # Test batch processing
        results = parser.parse_batch(test_documents)
        
        # Verify all documents were processed
        assert len(results) == 3
        assert results[0].title == "Document 1"
        assert results[1].title == "Document 2"
        assert results[2].title == "Document 3"
        
        # Verify content was extracted correctly
        assert "Content 1" in results[0].content
        assert "Content 2" in results[1].content
        assert "Content 3" in results[2].content

    def test_parse_handles_malformed_html(self):
        """
        üö´ Test parsing handles broken or malformed HTML gracefully.
        
        Real-world HTML is often broken - missing closing tags,
        nested incorrectly, or just plain wrong. Our parser should
        be robust enough to extract what it can.
        
        üí° Common HTML problems:
        - Unclosed tags (<p>text without closing)
        - Improperly nested tags (<b><i>text</b></i>)
        - Missing required elements (no <html> or <body>)
        - Invalid attributes or values
        """
        parser = DocumentParser()
        
        # Malformed HTML examples
        malformed_html_cases = [
            # Unclosed tags
            "<html><head><title>Test</title><body><p>Unclosed paragraph",
            
            # Improperly nested
            "<html><title>Test</title><body><b><i>Bold and italic</b></i></body></html>",
            
            # Missing structure
            "<p>Just a paragraph with no HTML structure</p>",
            
            # Mixed up tags
            "<html><body><title>Title in wrong place</title><p>Content</p></body></html>"
        ]
        
        for malformed_html in malformed_html_cases:
            # Should not crash, even with broken HTML
            result = parser.parse_html(malformed_html)
            
            # Should extract something useful
            assert result is not None
            assert result.content_type == "text/html"
            # Content might be empty or partial, but shouldn't crash

    @pytest.fixture
    def sample_documents(self):
        """
        üìã Fixture providing sample documents for testing.
        
        Fixtures are reusable test data that multiple tests can share.
        This avoids copying the same test data across many tests.
        
        üí° Why use fixtures:
        - Reduces code duplication
        - Ensures consistent test data
        - Makes tests easier to maintain
        - Provides realistic test scenarios
        """
        return {
            "simple_html": '''
                <html>
                <head><title>Simple Test</title></head>
                <body><p>Simple content for testing.</p></body>
                </html>
            ''',
            
            "complex_html": '''
                <html>
                <head>
                    <title>Complex Research Article</title>
                    <meta name="author" content="Research Team">
                </head>
                <body>
                    <nav>Navigation content</nav>
                    <main>
                        <article>
                            <h1>Research Findings</h1>
                            <p>Introduction paragraph...</p>
                            <h2>Methodology</h2>
                            <p>Methods used in this study...</p>
                            <h2>Results</h2>
                            <p>Key findings include...</p>
                        </article>
                    </main>
                    <footer>Footer content</footer>
                </body>
                </html>
            ''',
            
            "pdf_content": "This is sample text extracted from a PDF research paper. It contains methodology, results, and conclusions.",
        }

    def test_content_cleaning_removes_extra_whitespace(self, sample_documents):
        """
        üßπ Test that content cleaning removes extra whitespace and formatting.
        
        Raw extracted text often has extra spaces, line breaks, and
        formatting artifacts that make it hard to process.
        
        üí° Cleaning tasks:
        - Remove extra spaces and tabs
        - Normalize line breaks
        - Remove empty lines
        - Preserve paragraph structure
        """
        parser = DocumentParser(clean_content=True)
        
        # HTML with messy whitespace
        messy_html = '''
        <html>
        <head><title>Messy   Document</title></head>
        <body>
            <p>  This   has    extra     spaces.  </p>
            
            
            <p>
                
                This paragraph has
                weird line breaks.
                
            </p>
        </body>
        </html>
        '''
        
        result = parser.parse_html(messy_html)
        
        # Should clean up the content
        assert "extra     spaces" not in result.content  # Multiple spaces removed
        assert "This has extra spaces." in result.content  # Normalized spacing
        assert result.content.count('\n\n\n') == 0  # No triple line breaks
```

#### üü¢ GREEN Phase: Implement to Pass Tests

After writing tests, we'd implement the `DocumentParser`:

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
import re
from pathlib import Path
from bs4 import BeautifulSoup
import PyPDF2
import chardet
from datetime import datetime

@dataclass
class ParsedDocument:
    """Structured representation of a processed document."""
    title: Optional[str] = None
    content: str = ""
    source_url: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    content_type: str = "text/plain"

class DocumentParser:
    """Educational document parser for AI research systems."""
    
    def __init__(self, 
                 extract_images: bool = False,
                 preserve_formatting: bool = False,
                 max_content_length: Optional[int] = None,
                 extract_metadata: bool = True,
                 clean_content: bool = True):
        self.extract_images = extract_images
        self.preserve_formatting = preserve_formatting
        self.max_content_length = max_content_length
        self.extract_metadata = extract_metadata
        self.clean_content = clean_content
    
    def parse_html(self, html_content: str, source_url: Optional[str] = None) -> ParsedDocument:
        """Parse HTML content and extract structured information."""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Extract title
            title = self._extract_title(soup)
            
            # Extract main content
            content = self._extract_content(soup)
            
            # Clean content if requested
            if self.clean_content:
                content = self._clean_text(content)
            
            # Truncate if needed
            if self.max_content_length and len(content) > self.max_content_length:
                content = content[:self.max_content_length]
            
            # Extract metadata
            metadata = {}
            if self.extract_metadata:
                metadata = self._extract_html_metadata(soup, content)
            
            return ParsedDocument(
                title=title,
                content=content,
                source_url=source_url,
                metadata=metadata,
                content_type="text/html"
            )
            
        except Exception as e:
            return ParsedDocument(
                title="Error: Could not parse HTML",
                content="",
                source_url=source_url,
                metadata={"error": str(e)},
                content_type="text/html"
            )
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """Extract title from HTML, with fallbacks."""
        # Try title tag first
        title_tag = soup.find('title')
        if title_tag and title_tag.get_text().strip():
            return title_tag.get_text().strip()
        
        # Fallback to first h1
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
        
        # Fallback to meta title
        meta_title = soup.find('meta', attrs={'property': 'og:title'})
        if meta_title and meta_title.get('content'):
            return meta_title.get('content').strip()
        
        return None
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """Extract main content, avoiding navigation and ads."""
        # Remove unwanted elements
        for element in soup(['nav', 'footer', 'aside', 'script', 'style']):
            element.decompose()
        
        # Remove ads and navigation
        for element in soup.find_all(attrs={'class': re.compile(r'(ad|nav|menu|sidebar)', re.I)}):
            element.decompose()
        
        # Try to find main content area
        main_content = soup.find('main') or soup.find('article') or soup.find('body')
        
        if main_content:
            return main_content.get_text()
        else:
            return soup.get_text()
    
    def _clean_text(self, text: str) -> str:
        """Clean and normalize text content."""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove extra line breaks
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
        return text.strip()
    
    def _extract_html_metadata(self, soup: BeautifulSoup, content: str) -> Dict[str, Any]:
        """Extract metadata from HTML."""
        metadata = {}
        
        # Author
        author_tag = soup.find('meta', attrs={'name': 'author'})
        if author_tag and author_tag.get('content'):
            metadata['author'] = author_tag.get('content')
        
        # Description
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag and desc_tag.get('content'):
            metadata['description'] = desc_tag.get('content')
        
        # Keywords
        keywords_tag = soup.find('meta', attrs={'name': 'keywords'})
        if keywords_tag and keywords_tag.get('content'):
            metadata['keywords'] = keywords_tag.get('content')
        
        # Publication date
        date_tag = soup.find('meta', attrs={'name': 'publication-date'})
        if date_tag and date_tag.get('content'):
            metadata['publication_date'] = date_tag.get('content')
        
        # Computed metadata
        metadata['word_count'] = len(content.split())
        metadata['language'] = 'en'  # Simplified - would use language detection
        
        return metadata
    
    def parse_pdf(self, pdf_path: str) -> ParsedDocument:
        """Parse PDF file and extract text content."""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                # Extract text from all pages
                content = ""
                for page in pdf_reader.pages:
                    content += page.extract_text()
                
                # Clean content
                if self.clean_content:
                    content = self._clean_text(content)
                
                # Generate title from filename or first line
                title = Path(pdf_path).stem or content.split('\n')[0][:100]
                
                return ParsedDocument(
                    title=title,
                    content=content,
                    metadata={"source_file": pdf_path},
                    content_type="application/pdf"
                )
                
        except Exception as e:
            return ParsedDocument(
                title="Error: Could not parse PDF",
                content="",
                metadata={"error": str(e), "source_file": pdf_path},
                content_type="application/pdf"
            )
    
    def parse_batch(self, documents: List[Tuple[str, str]]) -> List[ParsedDocument]:
        """Process multiple documents in batch."""
        results = []
        for filename, content in documents:
            if filename.endswith('.html'):
                result = self.parse_html(content)
            elif filename.endswith('.pdf'):
                result = self.parse_pdf(content)  # Assuming content is file path for PDFs
            else:
                result = ParsedDocument(title=f"Unsupported: {filename}")
            
            results.append(result)
        
        return results
```

## üéØ Key Testing Concepts You Learned

### 1. **File I/O Testing**
- Use temporary files for testing file operations
- Mock file system operations to control test conditions
- Test different file formats and encodings

### 2. **Text Processing Testing**
- Test extraction of specific elements (titles, content, metadata)
- Verify cleaning and normalization of messy text
- Handle different character encodings and languages

### 3. **Error Handling Testing**
- Test corrupted files and malformed content
- Ensure graceful degradation instead of crashes
- Verify appropriate error messages and logging

### 4. **Batch Processing Testing**
- Test processing multiple documents efficiently
- Verify individual failures don't stop the entire batch
- Check memory usage and performance with large datasets

### 5. **Configuration Testing**
- Test different parser configuration options
- Verify behavior changes based on settings
- Test default values and edge cases

## üöÄ Practice Challenges

### Challenge 1: Test Markdown Processing
Write tests for parsing Markdown documents, including:
- Headers and formatting
- Links and images
- Code blocks and tables

### Challenge 2: Test Language Detection
Write tests that verify automatic language detection for documents in different languages.

### Challenge 3: Test Content Summarization
Write tests for a feature that automatically generates short summaries of long documents.

### Challenge 4: Test Duplicate Detection
Write tests that identify when the same content appears in multiple documents.

## üìö Real-World Applications

Document processing powers many systems you use every day:
- **Search engines** (indexing web pages for search)
- **Research platforms** (processing academic papers)
- **News aggregators** (extracting articles from various sites)
- **Legal systems** (processing contracts and court documents)
- **Medical systems** (processing patient records and research)
- **Educational platforms** (processing textbooks and learning materials)

## üí° Key Takeaways

1. **Real-world data is messy** - Always test with realistic, imperfect inputs
2. **Error handling is crucial** - Documents will be corrupted, missing, or malformed
3. **Performance matters** - Large documents and batch processing need special attention
4. **Metadata is valuable** - Don't just extract content, capture context too
5. **Encoding matters** - Support international text properly
6. **Clean extraction beats perfect parsing** - It's better to get good content than to crash

Remember: **Good document processors are like skilled librarians - they can find the valuable information even when it's buried in chaos!** üìÑ‚ú®
