# ü§ñ AI and Machine Learning Integration - Educational Module

## Welcome to AI Integration Testing!

Imagine you're building a smart research assistant that can read thousands of documents and then answer questions just like a knowledgeable professor. That's exactly what modern AI research systems do! But how do we test something as complex as artificial intelligence?

In this educational module, we'll learn how to test AI and Machine Learning components, understand how language models work, and explore how to build reliable AI-powered applications that middle schoolers (and everyone else!) can trust.

## üß† What is AI Integration in Research Systems?

AI Integration is like giving your computer a "smart brain" that can:
1. **Read and understand text** like humans do
2. **Answer complex questions** by combining information from multiple sources
3. **Generate summaries** of long documents
4. **Have conversations** that sound natural and helpful
5. **Learn from examples** to get better over time

### Real-World Example: From Raw Data to Smart Answers

When you ask an AI research system "What are the benefits of renewable energy?", here's what happens behind the scenes:

**Step 1: Understanding Your Question**
```python
# AI analyzes your question
user_question = "What are the benefits of renewable energy?"
ai_understands = {
    "topic": "renewable energy",
    "intent": "seeking benefits/advantages",
    "complexity": "intermediate",
    "expected_sources": ["scientific papers", "environmental reports"]
}
```

**Step 2: Finding Relevant Information**
```python
# System searches through thousands of documents
relevant_sources = [
    "Solar power reduces carbon emissions by 95%...",
    "Wind energy creates jobs in rural communities...",
    "Renewable sources are becoming cost-competitive..."
]
```

**Step 3: AI Generates Smart Answer**
```python
# AI combines sources into a comprehensive answer
ai_response = GenerationResponse(
    text="""Renewable energy offers several key benefits:
    1. Environmental: Reduces carbon emissions by up to 95%
    2. Economic: Creates jobs and reduces energy costs
    3. Energy Security: Reduces dependence on imports
    
    Based on recent studies from MIT and Stanford...""",
    sources=["Solar Study 2025", "Wind Energy Report"],
    confidence=0.92
)
```

Pretty amazing, right? But testing this is tricky because AI can be unpredictable!

## üß™ Testing AI Systems: Handling the Unpredictable

### Why Test AI Components?
AI systems are different from regular software because:
- **Responses vary**: The same question might get slightly different answers each time
- **Context matters**: Small changes in input can lead to big changes in output
- **Quality is subjective**: What makes a "good" answer?
- **External dependencies**: AI models are often hosted by third parties
- **Cost considerations**: Each AI call costs money and time
- **Bias and fairness**: AI can inadvertently favor certain viewpoints

Testing helps us build AI systems that are reliable, fair, and trustworthy.

### The TDD Approach for AI Integration

Let's learn by building comprehensive tests for an AI language model client:

#### üî¥ RED Phase: Write Failing Tests First

```python
import pytest
import asyncio
from typing import List, Dict, Any, Optional
from unittest.mock import Mock, patch, AsyncMock
from dataclasses import dataclass
from enum import Enum
import time

# These imports will fail initially - that's expected in TDD!
try:
    from src.llm_client import LLMClient, LLMConfig, GenerationResponse
    from src.llm_client import ChatMessage, MessageRole, TokenUsage
except ImportError:
    LLMClient = None
    LLMConfig = None
    GenerationResponse = None
    ChatMessage = None
    MessageRole = None
    TokenUsage = None

class TestAIIntegration:
    """
    ü§ñ Test suite for AI Language Model Integration.
    
    We're testing how our system communicates with AI models
    to generate intelligent responses for research questions.
    
    üéØ Learning Goals:
    - Understand how to test AI/ML components
    - Learn about mocking external AI services
    - Practice testing asynchronous AI operations
    - See how to validate AI response quality
    """
    
    def test_llm_config_holds_ai_settings(self):
        """
        ‚öôÔ∏è Test that LLMConfig stores all AI model settings properly.
        
        Think of LLMConfig like the "personality settings" for an AI.
        These settings control how the AI behaves and responds.
        
        üí° Key AI settings:
        - model_name: Which AI brain to use (GPT-4, Claude, etc.)
        - temperature: How creative vs consistent (0 = robotic, 1 = creative)
        - max_tokens: Maximum length of AI responses
        - top_p: How diverse the vocabulary should be
        - penalties: How much to avoid repetition
        """
        assert LLMConfig is not None, "LLMConfig class should exist"
        
        # Test creating AI configuration
        config = LLMConfig(
            model_name="gpt-4o-mini",  # A smart but affordable AI model
            temperature=0.3,  # Fairly consistent responses (good for research)
            max_tokens=1500,  # Allows for detailed but not too long answers
            top_p=0.9,  # Good vocabulary diversity
            frequency_penalty=0.1,  # Slightly avoid repetition
            presence_penalty=0.1,  # Encourage covering new topics
            timeout=30.0,  # 30 seconds max wait time
            api_key="test-key-12345",
            base_url="https://api.openai.com/v1"
        )
        
        # Verify all settings are stored correctly
        assert config.model_name == "gpt-4o-mini"
        assert config.temperature == 0.3
        assert config.max_tokens == 1500
        assert config.top_p == 0.9
        assert config.frequency_penalty == 0.1
        assert config.presence_penalty == 0.1
        assert config.timeout == 30.0
        assert config.api_key == "test-key-12345"
        assert config.base_url == "https://api.openai.com/v1"

    def test_llm_config_validates_settings(self):
        """
        ‚úÖ Test that LLMConfig rejects invalid AI settings.
        
        Just like you can't drive a car at -50 mph, AI models have
        limits on their settings. Our config should catch mistakes
        before they cause problems.
        
        üí° Common validation rules:
        - Model name can't be empty
        - Temperature must be between 0 and 2
        - Max tokens must be positive
        - Timeouts must be reasonable
        """
        # Test invalid model name
        with pytest.raises(ValueError, match="Model name cannot be empty"):
            LLMConfig(model_name="", temperature=0.7, max_tokens=1000)
        
        # Test invalid temperature (too low)
        with pytest.raises(ValueError, match="Temperature must be between 0 and 2"):
            LLMConfig(model_name="gpt-4", temperature=-0.1, max_tokens=1000)
        
        # Test invalid temperature (too high)
        with pytest.raises(ValueError, match="Temperature must be between 0 and 2"):
            LLMConfig(model_name="gpt-4", temperature=2.1, max_tokens=1000)
        
        # Test invalid max tokens
        with pytest.raises(ValueError, match="Max tokens must be positive"):
            LLMConfig(model_name="gpt-4", temperature=0.7, max_tokens=0)
        
        # Test invalid timeout
        with pytest.raises(ValueError, match="Timeout must be positive"):
            LLMConfig(model_name="gpt-4", temperature=0.7, max_tokens=1000, timeout=-5)

    def test_chat_message_represents_conversation(self):
        """
        üí¨ Test that ChatMessage can represent different parts of a conversation.
        
        AI conversations are like a play with different actors:
        - SYSTEM: The director giving instructions to the AI
        - USER: The person asking questions
        - ASSISTANT: The AI giving responses
        
        üí° Message structure:
        - role: Who is speaking (system/user/assistant)
        - content: What they're saying
        - metadata: Extra information (timestamps, IDs, etc.)
        - citations: Sources for the information
        """
        assert ChatMessage is not None, "ChatMessage class should exist"
        assert MessageRole is not None, "MessageRole enum should exist"
        
        # Test system message (instructions to AI)
        system_msg = ChatMessage(
            role=MessageRole.SYSTEM,
            content="You are a helpful research assistant for middle school students. Explain complex topics in simple terms with fun examples."
        )
        
        assert system_msg.role == MessageRole.SYSTEM
        assert "middle school students" in system_msg.content
        assert system_msg.metadata == {}  # Default empty metadata
        
        # Test user message (student's question)
        user_msg = ChatMessage(
            role=MessageRole.USER,
            content="How do solar panels work?",
            metadata={
                "student_id": "student_123",
                "grade_level": 7,
                "timestamp": "2025-07-31T10:30:00Z"
            }
        )
        
        assert user_msg.role == MessageRole.USER
        assert user_msg.content == "How do solar panels work?"
        assert user_msg.metadata["grade_level"] == 7
        
        # Test assistant message (AI's response)
        assistant_msg = ChatMessage(
            role=MessageRole.ASSISTANT,
            content="Solar panels work like plant leaves! They absorb sunlight and convert it into electricity through photovoltaic cells...",
            citations=[
                "National Renewable Energy Laboratory",
                "MIT Solar Energy Research"
            ],
            metadata={"confidence": 0.95, "response_time": 1.2}
        )
        
        assert assistant_msg.role == MessageRole.ASSISTANT
        assert "plant leaves" in assistant_msg.content  # Simple analogy
        assert len(assistant_msg.citations) == 2
        assert assistant_msg.metadata["confidence"] == 0.95

    def test_generation_response_contains_ai_output(self):
        """
        üìä Test that GenerationResponse captures everything about an AI response.
        
        When AI generates an answer, we need to track more than just the text.
        It's like keeping a detailed report card for each AI response.
        
        üí° Important response data:
        - text: The actual answer generated
        - finish_reason: Why the AI stopped (complete, length limit, etc.)
        - token_usage: How much "thinking" the AI did
        - model: Which AI brain was used
        - response_time: How long it took
        - citations: Sources the AI referenced
        """
        assert GenerationResponse is not None, "GenerationResponse class should exist"
        assert TokenUsage is not None, "TokenUsage class should exist"
        
        # Create a comprehensive AI response
        response = GenerationResponse(
            text="""Renewable energy is like switching from gas-powered cars to electric cars for our entire planet! 

Here are the main benefits:

üå± **Environmental Benefits:**
- Reduces pollution by up to 90%
- Helps fight climate change
- Keeps our air and water clean

üí∞ **Economic Benefits:**
- Creates millions of new jobs
- Saves money on electricity bills
- Makes energy prices more stable

üè† **Energy Independence:**
- Countries don't need to import as much oil
- Communities can generate their own power
- More reliable during emergencies

The best part? Renewable energy is getting cheaper every year while fossil fuels keep getting more expensive!""",
            
            finish_reason="stop",  # AI completed the response naturally
            
            token_usage=TokenUsage(
                prompt_tokens=85,  # Input question + instructions
                completion_tokens=145,  # AI response length
                total_tokens=230  # Total AI "thinking"
            ),
            
            model="gpt-4o-mini",
            response_time=2.3,  # Took 2.3 seconds to generate
            
            citations=[
                "International Renewable Energy Agency Report 2025",
                "MIT Clean Energy Research",
                "National Solar Foundation Study"
            ],
            
            metadata={
                "confidence": 0.93,
                "student_appropriate": True,
                "reading_level": 7
            }
        )
        
        # Verify all response data is captured correctly
        assert "electric cars" in response.text  # Contains simple analogy
        assert response.finish_reason == "stop"
        assert response.token_usage.total_tokens == 230
        assert response.model == "gpt-4o-mini"
        assert response.response_time == 2.3
        assert len(response.citations) == 3
        assert response.metadata["student_appropriate"] is True

    def test_token_usage_tracks_ai_consumption(self):
        """
        üî¢ Test that TokenUsage accurately tracks AI resource usage.
        
        Tokens are like "AI currency" - each word or piece of punctuation
        costs tokens. It's important to track this because:
        - AI calls cost real money
        - Too many tokens can slow down responses
        - We need to budget AI usage for schools
        
        üí° Token types:
        - prompt_tokens: The question and instructions we send
        - completion_tokens: The AI's response
        - total_tokens: prompt + completion (what we pay for)
        """
        usage = TokenUsage(
            prompt_tokens=120,  # Our question was 120 tokens
            completion_tokens=200,  # AI response was 200 tokens
            total_tokens=320  # Total cost: 320 tokens
        )
        
        # Verify token calculations
        assert usage.prompt_tokens == 120
        assert usage.completion_tokens == 200
        assert usage.total_tokens == 320
        assert usage.total_tokens == usage.prompt_tokens + usage.completion_tokens
        
        # Test cost estimation (approximate)
        estimated_cost = usage.estimate_cost(
            prompt_cost_per_1k_tokens=0.003,  # $0.003 per 1000 input tokens
            completion_cost_per_1k_tokens=0.006  # $0.006 per 1000 output tokens
        )
        
        # Should calculate: (120/1000 * 0.003) + (200/1000 * 0.006) = 0.00036 + 0.0012 = 0.00156
        assert abs(estimated_cost - 0.00156) < 0.0001  # Allow for rounding

    @pytest.mark.asyncio
    async def test_llm_client_can_be_created(self):
        """
        üèóÔ∏è Test that we can create an LLMClient for talking to AI.
        
        The LLMClient is like a translator that helps our program
        talk to AI models. It handles all the complicated communication
        so we can focus on asking good questions.
        
        üí° LLMClient responsibilities:
        - Connect to AI services safely
        - Format our questions properly
        - Handle AI responses
        - Manage errors and timeouts
        - Track usage and costs
        """
        assert LLMClient is not None, "LLMClient class should exist"
        
        # Create AI configuration
        config = LLMConfig(
            model_name="gpt-4o-mini",
            temperature=0.5,
            max_tokens=1000,
            api_key="test-key",
            timeout=30.0
        )
        
        # Create AI client
        client = LLMClient(config=config)
        
        # Verify client has necessary methods
        assert hasattr(client, 'generate_response'), "Should have generate_response method"
        assert hasattr(client, 'generate_summary'), "Should have generate_summary method"
        assert hasattr(client, 'generate_with_context'), "Should have generate_with_context method"
        assert hasattr(client, 'estimate_tokens'), "Should have estimate_tokens method"
        
        # Verify configuration is stored
        assert client.config.model_name == "gpt-4o-mini"
        assert client.config.temperature == 0.5

    @pytest.mark.asyncio
    async def test_generate_response_creates_ai_answer(self):
        """
        üéØ Test that generate_response gets AI to answer questions.
        
        This is the core functionality - asking AI a question and
        getting back a helpful, accurate answer with sources.
        
        üí° What we're testing:
        - AI receives our question correctly
        - AI provides a helpful response
        - Response includes proper citations
        - Token usage is tracked
        - Response time is reasonable
        """
        # Create test client with mocked AI service
        config = LLMConfig(model_name="gpt-4o-mini", temperature=0.3, max_tokens=1000)
        client = LLMClient(config=config)
        
        # Mock the AI service response
        mock_ai_response = {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": "Photosynthesis is like a solar panel in plants! Plants use sunlight, water, and carbon dioxide to make sugar (food) and oxygen. The green stuff in leaves (chlorophyll) captures sunlight energy, just like solar panels capture sun energy for electricity. This process is super important because it gives us oxygen to breathe and food to eat!"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 45,
                "completion_tokens": 78,
                "total_tokens": 123
            },
            "model": "gpt-4o-mini"
        }
        
        # Mock the AI API call
        with patch.object(client, '_call_ai_api', new_callable=AsyncMock) as mock_api:
            mock_api.return_value = mock_ai_response
            
            # Test generating a response
            question = "How does photosynthesis work?"
            sources = [
                "Biology textbook chapter on plant processes",
                "NASA Earth Science guide for students"
            ]
            
            response = await client.generate_response(
                question=question,
                sources=sources,
                student_level=7
            )
            
            # Verify the response
            assert isinstance(response, GenerationResponse)
            assert "solar panel" in response.text.lower()  # Should use analogy
            assert "chlorophyll" in response.text.lower()  # Should include key terms
            assert "oxygen" in response.text.lower()  # Should explain output
            assert response.finish_reason == "stop"
            assert response.token_usage.total_tokens == 123
            assert response.model == "gpt-4o-mini"
            assert len(response.citations) == 2
            
            # Verify API was called correctly
            mock_api.assert_called_once()
            call_args = mock_api.call_args[1]
            assert call_args["question"] == question
            assert call_args["sources"] == sources
            assert call_args["student_level"] == 7

    @pytest.mark.asyncio
    async def test_generate_summary_condenses_long_text(self):
        """
        üìù Test that generate_summary creates short versions of long content.
        
        Sometimes research documents are super long, but students need
        just the key points. The AI should create summaries that capture
        the most important information in fewer words.
        
        üí° Good summaries should:
        - Be much shorter than the original
        - Keep the most important points
        - Use simple language for students
        - Maintain accuracy
        - Include key statistics or facts
        """
        config = LLMConfig(model_name="gpt-4o-mini", temperature=0.2, max_tokens=500)
        client = LLMClient(config=config)
        
        # Long research text to summarize
        long_text = """
        Climate change refers to long-term shifts in global temperatures and weather patterns. 
        While climate change is a natural phenomenon, scientific evidence shows that human activities 
        have been the main driver of climate change since the 1800s, primarily through burning fossil 
        fuels like coal, oil and gas.
        
        Burning fossil fuels generates greenhouse gas emissions that act like a blanket wrapped around 
        the Earth, trapping the sun's heat and raising temperatures. The main greenhouse gases that 
        are causing climate change include carbon dioxide and methane. These come from using gasoline 
        for driving a car or coal for heating a building, for example. Clearing land and cutting down 
        forests can also release carbon dioxide. Agriculture, oil and gas operations are major sources 
        of methane emissions.
        
        Energy, industry, transport, buildings, agriculture and land use are among the main sectors 
        causing emissions. Global temperatures have already risen by about 1.1 degrees Celsius since 
        the late 1800s, and temperatures are projected to rise further. The last decade was the warmest 
        on record, and each of the last four decades has been successively warmer than any decade 
        that preceded it since 1850.
        
        Climate change can affect our health, ability to grow food, housing, safety and work. Some of 
        us are already more vulnerable to climate impacts, such as people living in small island 
        developing states. Sea level rise and saltwater intrusion is advancing in countries like the 
        Maldives and threatening the very existence of these nations.
        """
        
        # Mock AI response for summary
        mock_summary_response = {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": """Climate change is when Earth's weather patterns change over long periods. Here are the key points:

üå°Ô∏è **What's happening:** Earth is getting warmer - about 1.1¬∞C hotter since the 1800s
üè≠ **Main cause:** Humans burning fossil fuels (coal, oil, gas) creates greenhouse gases
ü´ß **How it works:** These gases trap heat like a blanket around Earth
üìà **Evidence:** The last 10 years were the warmest ever recorded
üåä **Effects:** Rising sea levels, changing weather, impacts on food and health
‚ö†Ô∏è **Who's affected most:** Small island nations like the Maldives could disappear underwater

The good news? We can help by using less fossil fuel energy and switching to renewable sources like solar and wind!"""
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 250,
                "completion_tokens": 135,
                "total_tokens": 385
            },
            "model": "gpt-4o-mini"
        }
        
        with patch.object(client, '_call_ai_api', new_callable=AsyncMock) as mock_api:
            mock_api.return_value = mock_summary_response
            
            # Test generating summary
            response = await client.generate_summary(
                text=long_text,
                max_length=200,  # Keep it short for students
                student_level=6
            )
            
            # Verify summary quality
            assert isinstance(response, GenerationResponse)
            assert len(response.text) < len(long_text)  # Should be much shorter
            assert "1.1¬∞C" in response.text  # Should keep key statistics
            assert "fossil fuels" in response.text.lower()  # Should keep key terms
            assert "renewable" in response.text.lower()  # Should suggest solutions
            assert "üå°Ô∏è" in response.text  # Should use emojis for students
            assert response.token_usage.total_tokens == 385

    @pytest.mark.asyncio
    async def test_generate_with_context_uses_research_sources(self):
        """
        üìö Test that generate_with_context combines multiple sources for better answers.
        
        The best research answers combine information from multiple reliable sources.
        This test verifies that our AI can read several documents and create
        a comprehensive answer that cites its sources properly.
        
        üí° Context-aware generation should:
        - Reference multiple provided sources
        - Synthesize information rather than just copying
        - Provide proper citations
        - Indicate confidence levels
        - Note if sources disagree
        """
        config = LLMConfig(model_name="gpt-4o-mini", temperature=0.4, max_tokens=800)
        client = LLMClient(config=config)
        
        # Multiple research sources with different perspectives
        research_context = [
            {
                "title": "NASA Earth Science Division - Solar Energy Basics",
                "content": "Solar energy works through photovoltaic cells that convert sunlight directly into electricity. Modern solar panels are about 20% efficient and can generate power for 25+ years.",
                "url": "https://nasa.gov/earth-science/solar-basics",
                "credibility": 0.98
            },
            {
                "title": "Department of Energy - Renewable Energy Benefits",
                "content": "Solar power installations have grown 4000% since 2010. Solar jobs are growing 5 times faster than the overall economy, with over 250,000 Americans working in solar.",
                "url": "https://energy.gov/renewable-benefits",
                "credibility": 0.95
            },
            {
                "title": "MIT Technology Review - Future of Solar",
                "content": "New perovskite solar cells could reach 35% efficiency by 2030. Cost of solar has dropped 90% in the last decade, making it the cheapest electricity source in many regions.",
                "url": "https://technologyreview.mit.edu/solar-future",
                "credibility": 0.92
            }
        ]
        
        # Mock comprehensive AI response
        mock_context_response = {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": """Solar energy is an amazing renewable technology that's transforming how we power our world! Here's what you need to know:

**How Solar Works:**
Solar panels use special cells called photovoltaic cells to turn sunlight directly into electricity - like how calculators work with tiny solar panels! NASA explains that modern panels are about 20% efficient, meaning they convert 1/5 of the sun's energy into usable power.

**Why Solar is Growing So Fast:**
According to the Department of Energy, solar power has grown by 4000% since 2010! That's incredible growth because:
- Solar costs have dropped 90% in the last decade (MIT Technology Review)
- It's now the cheapest electricity source in many places
- Solar jobs are growing 5x faster than other jobs (over 250,000 Americans work in solar!)

**The Future Looks Bright:**
MIT researchers are developing new solar cells that could be 35% efficient by 2030 - that's almost twice as good as today's panels!

**Sources:**
- NASA Earth Science Division: How solar panels work
- U.S. Department of Energy: Solar growth and job statistics  
- MIT Technology Review: Future solar technology improvements

Solar energy shows how science and technology can help solve real-world problems while creating good jobs!"""
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 180,
                "completion_tokens": 220,
                "total_tokens": 400
            },
            "model": "gpt-4o-mini"
        }
        
        with patch.object(client, '_call_ai_api', new_callable=AsyncMock) as mock_api:
            mock_api.return_value = mock_context_response
            
            # Test context-aware generation
            response = await client.generate_with_context(
                question="What are the benefits of solar energy?",
                context_sources=research_context,
                student_level=8,
                require_citations=True
            )
            
            # Verify comprehensive response
            assert isinstance(response, GenerationResponse)
            assert "photovoltaic" in response.text  # Technical accuracy
            assert "4000%" in response.text  # Specific statistics
            assert "NASA" in response.text  # Source attribution
            assert "Department of Energy" in response.text  # Multiple sources
            assert "MIT" in response.text  # Research institution
            assert len(response.citations) >= 3  # Multiple citations
            assert response.metadata.get("source_count", 0) == 3

    @pytest.mark.asyncio
    async def test_ai_handles_errors_gracefully(self):
        """
        üö® Test that AI integration handles errors without crashing.
        
        AI services can fail for many reasons - network problems, service
        outages, rate limits, or invalid requests. Our system should handle
        these gracefully and provide helpful error messages.
        
        üí° Common AI errors:
        - Rate limiting (too many requests)
        - Invalid API keys or authentication
        - Network timeouts or connectivity issues
        - Model overload or service downtime
        - Input validation errors (content too long, etc.)
        """
        config = LLMConfig(model_name="gpt-4o-mini", temperature=0.5, max_tokens=1000)
        client = LLMClient(config=config)
        
        # Test rate limiting error
        with patch.object(client, '_call_ai_api', new_callable=AsyncMock) as mock_api:
            mock_api.side_effect = Exception("Rate limit exceeded. Please try again later.")
            
            response = await client.generate_response(
                question="What is photosynthesis?",
                sources=["Biology textbook"]
            )
            
            # Should return error response instead of crashing
            assert isinstance(response, GenerationResponse)
            assert "error" in response.text.lower() or response.text == ""
            assert "rate limit" in response.metadata.get("error_message", "").lower()
            assert response.finish_reason == "error"
        
        # Test network timeout error
        with patch.object(client, '_call_ai_api', new_callable=AsyncMock) as mock_api:
            mock_api.side_effect = asyncio.TimeoutError("Request timed out after 30 seconds")
            
            response = await client.generate_response(
                question="Explain quantum physics",
                sources=["Physics journal"]
            )
            
            # Should handle timeout gracefully
            assert isinstance(response, GenerationResponse)
            assert response.finish_reason == "timeout"
            assert "timeout" in response.metadata.get("error_message", "").lower()
        
        # Test invalid API key error
        with patch.object(client, '_call_ai_api', new_callable=AsyncMock) as mock_api:
            mock_api.side_effect = Exception("Invalid API key provided")
            
            response = await client.generate_response(
                question="How do computers work?",
                sources=["Computer science guide"]
            )
            
            # Should indicate authentication problem
            assert isinstance(response, GenerationResponse)
            assert response.finish_reason == "auth_error"
            assert "api key" in response.metadata.get("error_message", "").lower()

    def test_estimate_tokens_predicts_ai_cost(self):
        """
        üí∞ Test that we can estimate token usage before making AI calls.
        
        Since AI calls cost money, it's helpful to estimate costs beforehand.
        This is especially important for schools with limited budgets.
        
        üí° Token estimation helps with:
        - Budgeting AI usage for classrooms
        - Choosing appropriate response lengths
        - Comparing costs of different approaches
        - Warning users about expensive operations
        """
        config = LLMConfig(model_name="gpt-4o-mini", temperature=0.5, max_tokens=1000)
        client = LLMClient(config=config)
        
        # Test estimating tokens for simple question
        simple_question = "What is gravity?"
        simple_sources = ["Physics textbook chapter 1"]
        
        estimated_tokens = client.estimate_tokens(
            question=simple_question,
            sources=simple_sources,
            expected_response_length="short"  # 100-200 words
        )
        
        # Should provide reasonable estimates
        assert estimated_tokens.prompt_tokens > 0
        assert estimated_tokens.estimated_completion_tokens > 0
        assert estimated_tokens.estimated_total_tokens > estimated_tokens.prompt_tokens
        assert estimated_tokens.estimated_cost > 0
        
        # Test estimating tokens for complex question
        complex_question = "Explain the complete process of photosynthesis, including light and dark reactions, and how it impacts global carbon cycles"
        complex_sources = [
            "Advanced Biology textbook - Chapter 8: Photosynthesis",
            "Scientific paper: Carbon cycle dynamics in forest ecosystems",
            "NASA Earth Science: Global carbon monitoring",
            "Research study: Photosynthetic efficiency in different plant species"
        ]
        
        complex_estimated = client.estimate_tokens(
            question=complex_question,
            sources=complex_sources,
            expected_response_length="detailed"  # 500-800 words
        )
        
        # Complex questions should cost more
        assert complex_estimated.estimated_total_tokens > estimated_tokens.estimated_total_tokens
        assert complex_estimated.estimated_cost > estimated_tokens.estimated_cost
        
        # Should provide cost warnings for expensive operations
        if complex_estimated.estimated_cost > 0.10:  # More than 10 cents
            assert "high_cost" in complex_estimated.warnings

    @pytest.mark.asyncio
    async def test_batch_processing_handles_multiple_questions(self):
        """
        üìö Test that we can efficiently process multiple questions at once.
        
        Teachers might want to generate answers for multiple student questions
        at the same time. Batch processing should be more efficient than
        processing each question individually.
        
        üí° Batch processing benefits:
        - Faster overall processing time
        - Better resource utilization
        - Consistent response quality
        - Lower per-question costs
        """
        config = LLMConfig(model_name="gpt-4o-mini", temperature=0.3, max_tokens=500)
        client = LLMClient(config=config)
        
        # Multiple student questions
        question_batch = [
            {
                "id": "q1",
                "question": "How do magnets work?",
                "sources": ["Physics for Kids - Magnetism Chapter"],
                "student_level": 6
            },
            {
                "id": "q2", 
                "question": "Why is the sky blue?",
                "sources": ["Optics and Light - Elementary Science"],
                "student_level": 5
            },
            {
                "id": "q3",
                "question": "How do plants make food?",
                "sources": ["Biology Basics - Plant Life"],
                "student_level": 7
            }
        ]
        
        # Mock batch responses
        mock_batch_responses = [
            {
                "id": "q1",
                "response": GenerationResponse(
                    text="Magnets work through invisible force fields called magnetic fields! Every magnet has two ends called poles - north and south. Opposite poles attract (pull together) while same poles repel (push apart). It's like how people with opposite personalities often become best friends!",
                    finish_reason="stop",
                    token_usage=TokenUsage(prompt_tokens=35, completion_tokens=45, total_tokens=80),
                    model="gpt-4o-mini",
                    response_time=1.1,
                    citations=["Physics for Kids - Magnetism Chapter"]
                )
            },
            {
                "id": "q2",
                "response": GenerationResponse(
                    text="The sky looks blue because of how sunlight bounces around in our atmosphere! Sunlight contains all colors (like a rainbow), but blue light gets scattered more than other colors when it hits tiny particles in the air. It's like when you shine a flashlight through fog - the light spreads out!",
                    finish_reason="stop",
                    token_usage=TokenUsage(prompt_tokens=32, completion_tokens=52, total_tokens=84),
                    model="gpt-4o-mini",
                    response_time=1.3,
                    citations=["Optics and Light - Elementary Science"]
                )
            },
            {
                "id": "q3",
                "response": GenerationResponse(
                    text="Plants make their own food through photosynthesis - it's like having a kitchen in every leaf! Plants use sunlight, water from their roots, and carbon dioxide from the air to cook up sugar (their food). The green stuff in leaves (chlorophyll) is like the chef that makes it all happen!",
                    finish_reason="stop",
                    token_usage=TokenUsage(prompt_tokens=38, completion_tokens=48, total_tokens=86),
                    model="gpt-4o-mini", 
                    response_time=1.2,
                    citations=["Biology Basics - Plant Life"]
                )
            }
        ]
        
        with patch.object(client, '_process_batch', new_callable=AsyncMock) as mock_batch:
            mock_batch.return_value = mock_batch_responses
            
            # Test batch processing
            results = await client.generate_batch_responses(question_batch)
            
            # Verify all questions were processed
            assert len(results) == 3
            assert all(result["id"] in ["q1", "q2", "q3"] for result in results)
            
            # Verify response quality
            q1_response = next(r for r in results if r["id"] == "q1")["response"]
            assert "magnetic fields" in q1_response.text.lower()
            assert "north and south" in q1_response.text.lower()
            
            q2_response = next(r for r in results if r["id"] == "q2")["response"]
            assert "sunlight" in q2_response.text.lower()
            assert "scattered" in q2_response.text.lower()
            
            q3_response = next(r for r in results if r["id"] == "q3")["response"]
            assert "photosynthesis" in q3_response.text.lower()
            assert "chlorophyll" in q3_response.text.lower()
            
            # Verify efficiency gains
            total_individual_time = sum(r["response"].response_time for r in results)
            batch_processing_time = 2.5  # Simulated batch time
            assert batch_processing_time < total_individual_time  # Should be faster

    def test_response_quality_assessment(self):
        """
        üìä Test that we can assess the quality of AI responses.
        
        Not all AI responses are equally good. We need ways to measure
        response quality so teachers can trust the answers students receive.
        
        üí° Quality metrics:
        - Accuracy: Is the information correct?
        - Completeness: Does it answer the full question?
        - Clarity: Is it understandable for the target age group?
        - Citations: Are sources properly referenced?
        - Safety: Is the content appropriate for students?
        """
        config = LLMConfig(model_name="gpt-4o-mini", temperature=0.3, max_tokens=1000)
        client = LLMClient(config=config)
        
        # Test high-quality response
        good_response = GenerationResponse(
            text="""Photosynthesis is how plants make their own food using sunlight! Here's how it works:

**What plants need:**
- Sunlight (energy source)
- Water (from roots)
- Carbon dioxide (from air)

**What happens:**  
1. Chlorophyll (green stuff in leaves) captures sunlight energy
2. Plants combine water and CO2 using this energy
3. This creates glucose (sugar) for plant food
4. Oxygen is released as a bonus for us to breathe!

**Why it matters:**
- Gives us oxygen to breathe
- Removes CO2 from air (helps climate)
- Feeds almost all life on Earth (food chains start with plants)

Think of leaves as tiny solar-powered kitchens cooking sugar all day long!""",
            
            finish_reason="stop",
            token_usage=TokenUsage(prompt_tokens=40, completion_tokens=160, total_tokens=200),
            model="gpt-4o-mini",
            response_time=1.8,
            citations=["Elementary Biology Textbook", "NASA Earth Science for Students"],
            metadata={"confidence": 0.94}
        )
        
        # Test response quality
        quality_score = client.assess_response_quality(
            response=good_response,
            original_question="How does photosynthesis work?",
            target_grade_level=6,
            required_topics=["sunlight", "water", "carbon dioxide", "oxygen", "chlorophyll"]
        )
        
        # Should score highly on all metrics
        assert quality_score.overall_score >= 0.85  # High overall quality
        assert quality_score.accuracy >= 0.90  # Scientifically accurate
        assert quality_score.completeness >= 0.85  # Covers main topics
        assert quality_score.clarity >= 0.90  # Age-appropriate language
        assert quality_score.citation_quality >= 0.80  # Good sources
        assert quality_score.safety_score >= 0.95  # Appropriate content
        
        # Test poor-quality response
        poor_response = GenerationResponse(
            text="Plants do photosynthesis. It involves light and stuff. They make food somehow. The end.",
            finish_reason="stop", 
            token_usage=TokenUsage(prompt_tokens=40, completion_tokens=15, total_tokens=55),
            model="gpt-4o-mini",
            response_time=0.5,
            citations=[],  # No citations
            metadata={"confidence": 0.45}
        )
        
        poor_quality_score = client.assess_response_quality(
            response=poor_response,
            original_question="How does photosynthesis work?",
            target_grade_level=6,
            required_topics=["sunlight", "water", "carbon dioxide", "oxygen", "chlorophyll"]
        )
        
        # Should score poorly
        assert poor_quality_score.overall_score < 0.50  # Low overall quality
        assert poor_quality_score.completeness < 0.30  # Missing key topics
        assert poor_quality_score.citation_quality < 0.20  # No sources
        assert len(poor_quality_score.improvement_suggestions) > 0  # Should suggest improvements
```

#### üü¢ GREEN Phase: Implement to Pass Tests

After writing tests, we'd implement the AI integration classes:

```python
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, AsyncIterator
from enum import Enum
import asyncio
import aiohttp
import json
import time
import re

class MessageRole(Enum):
    """Roles in AI conversation"""
    SYSTEM = "system"
    USER = "user" 
    ASSISTANT = "assistant"

@dataclass
class TokenUsage:
    """Track AI token consumption and costs"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    
    def estimate_cost(self, prompt_cost_per_1k_tokens: float, completion_cost_per_1k_tokens: float) -> float:
        """Estimate cost based on token usage"""
        prompt_cost = (self.prompt_tokens / 1000) * prompt_cost_per_1k_tokens
        completion_cost = (self.completion_tokens / 1000) * completion_cost_per_1k_tokens
        return prompt_cost + completion_cost

@dataclass
class ChatMessage:
    """Represent a message in AI conversation"""
    role: MessageRole
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    citations: List[str] = field(default_factory=list)

@dataclass 
class GenerationResponse:
    """AI generation response with metadata"""
    text: str
    finish_reason: str
    token_usage: TokenUsage
    model: str
    response_time: float
    citations: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class LLMConfig:
    """Configuration for AI language model"""
    model_name: str
    temperature: float
    max_tokens: int
    top_p: float = 0.9
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    timeout: float = 30.0
    api_key: Optional[str] = None
    base_url: str = "https://api.openai.com/v1"
    
    def __post_init__(self):
        """Validate configuration parameters"""
        if not self.model_name:
            raise ValueError("Model name cannot be empty")
        if not 0 <= self.temperature <= 2:
            raise ValueError("Temperature must be between 0 and 2")
        if self.max_tokens <= 0:
            raise ValueError("Max tokens must be positive")
        if self.timeout <= 0:
            raise ValueError("Timeout must be positive")

class LLMClient:
    """Educational AI client for research systems"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self._session = None
    
    async def generate_response(self, question: str, sources: List[str], 
                              student_level: int = 7) -> GenerationResponse:
        """Generate AI response to research question"""
        try:
            start_time = time.time()
            
            # Build context-aware prompt
            system_prompt = f"""You are a helpful research assistant for grade {student_level} students. 
            Explain complex topics using simple language, analogies, and examples appropriate for their age.
            Always cite your sources and use engaging, educational language."""
            
            user_prompt = f"""Question: {question}

Available sources:
{chr(10).join(f"- {source}" for source in sources)}

Please provide a comprehensive, age-appropriate answer with proper citations."""
            
            # Call AI API
            ai_response = await self._call_ai_api(
                question=question,
                sources=sources,
                student_level=student_level,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            response_time = time.time() - start_time
            
            # Extract response data
            message = ai_response["choices"][0]["message"]
            usage = ai_response["usage"]
            
            return GenerationResponse(
                text=message["content"],
                finish_reason=ai_response["choices"][0]["finish_reason"],
                token_usage=TokenUsage(
                    prompt_tokens=usage["prompt_tokens"],
                    completion_tokens=usage["completion_tokens"],
                    total_tokens=usage["total_tokens"]
                ),
                model=ai_response["model"],
                response_time=response_time,
                citations=sources,
                metadata={"student_level": student_level}
            )
            
        except asyncio.TimeoutError:
            return GenerationResponse(
                text="",
                finish_reason="timeout",
                token_usage=TokenUsage(0, 0, 0),
                model=self.config.model_name,
                response_time=self.config.timeout,
                metadata={"error_message": "Request timed out"}
            )
        except Exception as e:
            error_type = "auth_error" if "api key" in str(e).lower() else "error"
            return GenerationResponse(
                text="",
                finish_reason=error_type,
                token_usage=TokenUsage(0, 0, 0),
                model=self.config.model_name,
                response_time=0,
                metadata={"error_message": str(e)}
            )
    
    async def generate_summary(self, text: str, max_length: int = 200, 
                             student_level: int = 7) -> GenerationResponse:
        """Generate student-friendly summary of long text"""
        # Implementation would create concise summaries
        pass
    
    async def generate_with_context(self, question: str, context_sources: List[Dict],
                                  student_level: int = 7, require_citations: bool = True) -> GenerationResponse:
        """Generate response using multiple research sources"""
        # Implementation would synthesize multiple sources
        pass
    
    def estimate_tokens(self, question: str, sources: List[str], 
                       expected_response_length: str = "medium") -> 'TokenEstimate':
        """Estimate token usage before making AI call"""
        # Simple estimation logic
        prompt_length = len(question) + sum(len(s) for s in sources)
        estimated_prompt_tokens = prompt_length // 4  # Rough approximation
        
        length_multipliers = {"short": 50, "medium": 150, "detailed": 400}
        estimated_completion = length_multipliers.get(expected_response_length, 150)
        
        total_estimated = estimated_prompt_tokens + estimated_completion
        estimated_cost = total_estimated * 0.00001  # Rough cost estimate
        
        return TokenEstimate(
            prompt_tokens=estimated_prompt_tokens,
            estimated_completion_tokens=estimated_completion,
            estimated_total_tokens=total_estimated,
            estimated_cost=estimated_cost
        )
    
    def assess_response_quality(self, response: GenerationResponse, original_question: str,
                              target_grade_level: int, required_topics: List[str]) -> 'QualityScore':
        """Assess the quality of an AI response"""
        # Implementation would analyze response quality
        pass
    
    async def _call_ai_api(self, **kwargs) -> Dict[str, Any]:
        """Make actual API call to AI service"""
        # Implementation would handle HTTP requests to AI API
        pass
```

## üéØ Key Testing Concepts You Learned

### 1. **Mocking External Services**
- Use `AsyncMock` for asynchronous AI API calls
- Mock responses to test different scenarios
- Control external dependencies for reliable tests

### 2. **Asynchronous Testing**
- Use `@pytest.mark.asyncio` for async tests
- Test timeout handling and error scenarios
- Verify async operations complete correctly

### 3. **Configuration Testing**
- Test parameter validation and constraints
- Verify different configuration combinations
- Test default values and edge cases

### 4. **Cost and Performance Testing**
- Track token usage and estimated costs
- Test batch processing efficiency
- Verify response time requirements

### 5. **Quality Assessment Testing**
- Test response accuracy and completeness
- Verify age-appropriate language
- Check citation quality and safety

## üöÄ Practice Challenges

### Challenge 1: Test Multi-Language Support
Write tests for AI responses in different languages (Spanish, French, etc.).

### Challenge 2: Test Content Filtering
Write tests that verify inappropriate content is filtered out of AI responses.

### Challenge 3: Test Conversation Memory
Write tests for AI that remembers previous questions in a conversation.

### Challenge 4: Test Custom Instructions
Write tests that verify AI follows specific teaching guidelines or curriculum requirements.

## üìö Real-World Applications

AI integration powers educational systems worldwide:
- **Personalized tutoring** (AI adapts to each student's learning style)
- **Homework assistance** (AI helps students understand concepts)
- **Language learning** (AI provides conversation practice)
- **Research assistance** (AI helps students find and analyze information)
- **Accessibility support** (AI helps students with different learning needs)
- **Teacher tools** (AI helps create lesson plans and assessments)

## üí° Key Takeaways

1. **AI is unpredictable** - Always test multiple scenarios and edge cases
2. **Mock external services** - Don't rely on real AI APIs for testing
3. **Track costs carefully** - AI usage can get expensive quickly
4. **Quality matters more than speed** - Good answers are worth waiting for
5. **Error handling is critical** - AI services will fail, be prepared
6. **Student safety first** - Always filter and validate AI responses
7. **Transparency builds trust** - Show sources and confidence levels

Remember: **Good AI integration is like having a knowledgeable teaching assistant - helpful, accurate, and always putting student learning first!** ü§ñ‚ú®
